{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zz7OMjnBMEEc",
        "outputId": "8b5da3b2-b20d-4246-f135-05d58b294bca",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------------+---------+---+-------------+---------------+--------------+-----------+------+\n",
            "|Customer ID|  Customer Name|  Segment|Age|      Country|           City|         State|Postal Code|Region|\n",
            "+-----------+---------------+---------+---+-------------+---------------+--------------+-----------+------+\n",
            "|   CG-12520|    Claire Gute| Consumer| 67|United States|      Henderson|      Kentucky|      42420| South|\n",
            "|   DV-13045|Darrin Van Huff|Corporate| 31|United States|    Los Angeles|    California|      90036|  West|\n",
            "|   SO-20335| Sean O'Donnell| Consumer| 65|United States|Fort Lauderdale|       Florida|      33311| South|\n",
            "|   BH-11710|Brosina Hoffman| Consumer| 20|United States|    Los Angeles|    California|      90032|  West|\n",
            "|   AA-10480|   Andrew Allen| Consumer| 50|United States|        Concord|North Carolina|      28027| South|\n",
            "+-----------+---------------+---------+---+-------------+---------------+--------------+-----------+------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---------------+---------------+------------+--------------------+\n",
            "|     Product ID|       Category|Sub-Category|        Product Name|\n",
            "+---------------+---------------+------------+--------------------+\n",
            "|FUR-BO-10001798|      Furniture|   Bookcases|Bush Somerset Col...|\n",
            "|FUR-CH-10000454|      Furniture|      Chairs|Hon Deluxe Fabric...|\n",
            "|OFF-LA-10000240|Office Supplies|      Labels|Self-Adhesive Add...|\n",
            "|FUR-TA-10000577|      Furniture|      Tables|Bretford CR4500 S...|\n",
            "|OFF-ST-10000760|Office Supplies|     Storage|Eldon Fold N Roll...|\n",
            "+---------------+---------------+------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+----------+--------------+----------+----------+--------------+-----------+---------------+--------+--------+--------+--------+\n",
            "|Order Line|      Order ID|Order Date| Ship Date|     Ship Mode|Customer ID|     Product ID|   Sales|Quantity|Discount|  Profit|\n",
            "+----------+--------------+----------+----------+--------------+-----------+---------------+--------+--------+--------+--------+\n",
            "|         1|CA-2016-152156|2016-11-08|2016-11-11|  Second Class|   CG-12520|FUR-BO-10001798|  261.96|       2|     0.0| 41.9136|\n",
            "|         2|CA-2016-152156|2016-11-08|2016-11-11|  Second Class|   CG-12520|FUR-CH-10000454|  731.94|       3|     0.0| 219.582|\n",
            "|         3|CA-2016-138688|2016-06-12|2016-06-16|  Second Class|   DV-13045|OFF-LA-10000240|   14.62|       2|     0.0|  6.8714|\n",
            "|         4|US-2015-108966|2015-10-11|2015-10-18|Standard Class|   SO-20335|FUR-TA-10000577|957.5775|       5|    0.45|-383.031|\n",
            "|         5|US-2015-108966|2015-10-11|2015-10-18|Standard Class|   SO-20335|OFF-ST-10000760|  22.368|       2|     0.2|  2.5164|\n",
            "+----------+--------------+----------+----------+--------------+-----------+---------------+--------+--------+--------+--------+\n",
            "only showing top 5 rows\n",
            "\n",
            "2025-05-09 09:52:23,308 - INFO - Customer data loaded successfully\n",
            "2025-05-09 09:52:24,282 - INFO - Product data loaded successfully\n",
            "2025-05-09 09:52:25,218 - INFO - Sales data loaded successfully\n",
            "2025-05-09 09:52:25,219 - INFO - Customer DataFrame\n",
            "2025-05-09 09:52:25,694 - INFO - Products DataFrame\n",
            "2025-05-09 09:52:26,142 - INFO - Sales DataFrame\n"
          ]
        }
      ],
      "source": [
        "################ Dmart analysis using pyspark #####################\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import logging\n",
        "from functools import reduce\n",
        "from pyspark.sql.functions import col,current_date,to_date\n",
        "\n",
        "\n",
        "# Clean up any existing handlers (important in Colab!)\n",
        "for handler in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(handler)\n",
        "\n",
        "# --- Setup Logging ---\n",
        "log_file = 'dmart.log'\n",
        "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "\n",
        "# Creating Spark session\n",
        "spark = SparkSession.builder.appName(\"Dmart analysis using pyspark\").getOrCreate()\n",
        "\n",
        "\n",
        "# Loading CSVs into PySpark DataFrames\n",
        "\n",
        "# Trying to load customer csv file\n",
        "try:\n",
        "  customer_df = spark.read.csv(\"/content/Customer.csv\", header=True, inferSchema=True)\n",
        "  logging.info(\"Customer data loaded successfully\")\n",
        "except Exception as e:\n",
        "  logging.error(f\"Error loading Customer data: {e}\")\n",
        "\n",
        "# Trying to load products csv file\n",
        "try:\n",
        "  products_df = spark.read.csv(\"/content/Product.csv\", header=True, inferSchema=True)\n",
        "  logging.info(\"Product data loaded successfully\")\n",
        "except Exception as e:\n",
        "  logging.error(f\"Error loading Product data: {e}\")\n",
        "\n",
        "# Trying to load sales csv file\n",
        "try:\n",
        "  sales_df = spark.read.csv(\"/content/Sales.csv\", header=True, inferSchema=True)\n",
        "  logging.info(\"Sales data loaded successfully\")\n",
        "except Exception as e:\n",
        "  logging.error(f\"Error loading Sales data: {e}\")\n",
        "\n",
        "\n",
        "# Preview dataframes\n",
        "if customer_df:\n",
        "    logger.info(\"Customer DataFrame\")\n",
        "    customer_df.show(5)\n",
        "\n",
        "if products_df:\n",
        "    logger.info(\"Products DataFrame\")\n",
        "    products_df.show(5)\n",
        "\n",
        "if sales_df:\n",
        "    logger.info(\"Sales DataFrame\")\n",
        "    sales_df.show(5)\n",
        "\n",
        "# --- Show log file contents ---\n",
        "!cat dmart.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddZRFR0CO0OD",
        "outputId": "d94d4003-b5e8-4dc5-b00a-249a1c330dd5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "793"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "customer_df.count()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s0kVQXywjAg",
        "outputId": "d619e58f-b15e-40ad-fb4b-e564fcea072b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1862"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "products_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Isj_gS4wnPR",
        "outputId": "7fa58e31-78a6-41ab-d09f-caacd32e8ca1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9994"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sales_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5LP0EHAfw20N"
      },
      "outputs": [],
      "source": [
        "################# Data Cleaning and Transformation#####################\n",
        "\n",
        "# Remove duplicates\n",
        "customer_df = customer_df.dropDuplicates()\n",
        "products_df = products_df.dropDuplicates()\n",
        "sales_df = sales_df.dropDuplicates()\n",
        "logging.info(\"Duplicates removed successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "R8EaXp46yM5L",
        "outputId": "1e4f4aa0-8dca-4d4d-d8b1-4b9788baba9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Customer ID: string (nullable = true)\n",
            " |-- Customer Name: string (nullable = true)\n",
            " |-- Segment: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            " |-- Postal Code: integer (nullable = true)\n",
            " |-- Region: string (nullable = true)\n",
            "\n",
            "+-----------+-----------------+-----------+---+-------------+------------+------------+-----------+-------+\n",
            "|Customer ID|    Customer Name|    Segment|Age|      Country|        City|       State|Postal Code| Region|\n",
            "+-----------+-----------------+-----------+---+-------------+------------+------------+-----------+-------+\n",
            "|   PO-19180|Philisse Overcash|Home Office| 46|United States|     Chicago|    Illinois|      60623|Central|\n",
            "|   MD-17860|Michael Dominguez|  Corporate| 21|United States|Indianapolis|     Indiana|      46203|Central|\n",
            "|   AA-10645|    Anna Andreadi|   Consumer| 32|United States|     Chester|Pennsylvania|      19013|   East|\n",
            "|   DB-13615|    Doug Bickford|   Consumer| 19|United States| Los Angeles|  California|      90045|   West|\n",
            "|   KD-16615|         Ken Dana|  Corporate| 41|United States|  Scottsdale|     Arizona|      85254|   West|\n",
            "+-----------+-----------------+-----------+---+-------------+------------+------------+-----------+-------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- Product ID: string (nullable = true)\n",
            " |-- Category: string (nullable = true)\n",
            " |-- Sub-Category: string (nullable = true)\n",
            " |-- Product Name: string (nullable = true)\n",
            "\n",
            "+---------------+---------------+------------+--------------------+\n",
            "|     Product ID|       Category|Sub-Category|        Product Name|\n",
            "+---------------+---------------+------------+--------------------+\n",
            "|TEC-AC-10001767|     Technology| Accessories|SanDisk Ultra 64 ...|\n",
            "|OFF-EN-10002504|Office Supplies|   Envelopes|Tyvek  Top-Openin...|\n",
            "|OFF-PA-10001937|Office Supplies|       Paper|            Xerox 21|\n",
            "|OFF-PA-10003039|Office Supplies|       Paper|          Xerox 1960|\n",
            "|OFF-AR-10001953|Office Supplies|         Art|Boston 1645 Delux...|\n",
            "+---------------+---------------+------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- Order Line: integer (nullable = true)\n",
            " |-- Order ID: string (nullable = true)\n",
            " |-- Order Date: date (nullable = true)\n",
            " |-- Ship Date: date (nullable = true)\n",
            " |-- Ship Mode: string (nullable = true)\n",
            " |-- Customer ID: string (nullable = true)\n",
            " |-- Product ID: string (nullable = true)\n",
            " |-- Sales: double (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- Discount: double (nullable = true)\n",
            " |-- Profit: double (nullable = true)\n",
            "\n",
            "+----------+--------------+----------+----------+--------------+-----------+---------------+-------+--------+--------+----------+\n",
            "|Order Line|      Order ID|Order Date| Ship Date|     Ship Mode|Customer ID|     Product ID|  Sales|Quantity|Discount|    Profit|\n",
            "+----------+--------------+----------+----------+--------------+-----------+---------------+-------+--------+--------+----------+\n",
            "|       128|US-2017-107272|2017-11-05|2017-11-12|Standard Class|   TS-21610|OFF-ST-10002974|243.992|       7|     0.2|    30.499|\n",
            "|       339|CA-2014-129924|2014-07-12|2014-07-17|Standard Class|   AC-10420|FUR-TA-10004575|698.352|       3|     0.2|  -17.4588|\n",
            "|       590|US-2016-156986|2016-03-20|2016-03-24|Standard Class|   ZC-21910|OFF-PA-10002005| 20.736|       4|     0.2|    7.2576|\n",
            "|       684|US-2017-168116|2017-11-04|2017-11-04|      Same Day|   GT-14635|TEC-MA-10004125|7999.98|       4|     0.5|-3839.9904|\n",
            "|       739|CA-2014-132500|2014-09-08|2014-09-12|Standard Class|   GZ-14470|TEC-AC-10001383|  49.98|       2|     0.0|    8.4966|\n",
            "+----------+--------------+----------+----------+--------------+-----------+---------------+-------+--------+--------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Correct data types (e.g., ensuring Order date is in a Date format, Quantity is an integer, etc.).\n",
        "customer_df.printSchema()\n",
        "customer_df.show(5)\n",
        "\n",
        "products_df.printSchema()\n",
        "products_df.show(5)\n",
        "\n",
        "sales_df.printSchema()\n",
        "sales_df.show(5)\n",
        "\n",
        "logging.info(\"Data types corrected successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuV-0sfT0gyh",
        "outputId": "f85c6fcb-3700-486e-a062-d193becc9e8d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------+---+-------+----+-----+-----------+------+\n",
            "|Customer ID|Customer Name|Segment|Age|Country|City|State|Postal Code|Region|\n",
            "+-----------+-------------+-------+---+-------+----+-----+-----------+------+\n",
            "+-----------+-------------+-------+---+-------+----+-----+-----------+------+\n",
            "\n",
            "+----------+--------+------------+------------+\n",
            "|Product ID|Category|Sub-Category|Product Name|\n",
            "+----------+--------+------------+------------+\n",
            "+----------+--------+------------+------------+\n",
            "\n",
            "+----------+--------+----------+---------+---------+-----------+----------+-----+--------+--------+------+\n",
            "|Order Line|Order ID|Order Date|Ship Date|Ship Mode|Customer ID|Product ID|Sales|Quantity|Discount|Profit|\n",
            "+----------+--------+----------+---------+---------+-----------+----------+-----+--------+--------+------+\n",
            "+----------+--------+----------+---------+---------+-----------+----------+-----+--------+--------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Handle missing or null values by either dropping or filling them.\n",
        "\n",
        "# Identifying the null values\n",
        "customer_df_with_missing_values = customer_df.filter(\n",
        "    reduce(lambda a, b: a | b, [col(c).isNull() for c in customer_df.columns])\n",
        ")\n",
        "customer_df_with_missing_values.show()\n",
        "\n",
        "products_df_with_missing_values = products_df.filter(\n",
        "    reduce(lambda a, b: a | b, [col(c).isNull() for c in products_df.columns])\n",
        ")\n",
        "products_df_with_missing_values.show()\n",
        "\n",
        "sales_df_with_missing_values = sales_df.filter(\n",
        "    reduce(lambda a, b: a | b, [col(c).isNull() for c in sales_df.columns])\n",
        ")\n",
        "sales_df_with_missing_values.show()\n",
        "\n",
        "logging.info(\"Null values identified successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xNKtHBwE2Uac"
      },
      "outputs": [],
      "source": [
        "# Dropping the null rows\n",
        "\n",
        "customer_df=customer_df.dropna()\n",
        "products_df=products_df.dropna()\n",
        "sales_df=sales_df.dropna()\n",
        "\n",
        "logging.info(\"Null values removed successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Renaming the column names to snake-case for consistency\n",
        "\n",
        "customer_df = customer_df.select([col(c).alias(c.lower().replace(' ', '_')) for c in customer_df.columns])\n",
        "customer_df.show(5)\n",
        "products_df = products_df.select([col(c).alias(c.lower().replace(' ', '_').replace('-', '_')) for c in products_df.columns])\n",
        "products_df.show(5)\n",
        "sales_df = sales_df.select([col(c).alias(c.lower().replace(' ', '_')) for c in sales_df.columns])\n",
        "sales_df.show(5)\n",
        "\n",
        "logging.info(\"Column names renamed successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5JCZZEZCcwkI",
        "outputId": "668a7350-a364-4469-cc95-fc7de03b9088"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------------+-----------+---+-------------+------------+------------+-----------+-------+\n",
            "|customer_id|    customer_name|    segment|age|      country|        city|       state|postal_code| region|\n",
            "+-----------+-----------------+-----------+---+-------------+------------+------------+-----------+-------+\n",
            "|   PO-19180|Philisse Overcash|Home Office| 46|United States|     Chicago|    Illinois|      60623|Central|\n",
            "|   MD-17860|Michael Dominguez|  Corporate| 21|United States|Indianapolis|     Indiana|      46203|Central|\n",
            "|   AA-10645|    Anna Andreadi|   Consumer| 32|United States|     Chester|Pennsylvania|      19013|   East|\n",
            "|   DB-13615|    Doug Bickford|   Consumer| 19|United States| Los Angeles|  California|      90045|   West|\n",
            "|   KD-16615|         Ken Dana|  Corporate| 41|United States|  Scottsdale|     Arizona|      85254|   West|\n",
            "+-----------+-----------------+-----------+---+-------------+------------+------------+-----------+-------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---------------+---------------+------------+--------------------+\n",
            "|     product_id|       category|sub_category|        product_name|\n",
            "+---------------+---------------+------------+--------------------+\n",
            "|TEC-AC-10001767|     Technology| Accessories|SanDisk Ultra 64 ...|\n",
            "|OFF-EN-10002504|Office Supplies|   Envelopes|Tyvek  Top-Openin...|\n",
            "|OFF-PA-10001937|Office Supplies|       Paper|            Xerox 21|\n",
            "|OFF-PA-10003039|Office Supplies|       Paper|          Xerox 1960|\n",
            "|OFF-AR-10001953|Office Supplies|         Art|Boston 1645 Delux...|\n",
            "+---------------+---------------+------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+----------+--------------+----------+----------+--------------+-----------+---------------+-------+--------+--------+----------+\n",
            "|order_line|      order_id|order_date| ship_date|     ship_mode|customer_id|     product_id|  sales|quantity|discount|    profit|\n",
            "+----------+--------------+----------+----------+--------------+-----------+---------------+-------+--------+--------+----------+\n",
            "|       128|US-2017-107272|2017-11-05|2017-11-12|Standard Class|   TS-21610|OFF-ST-10002974|243.992|       7|     0.2|    30.499|\n",
            "|       339|CA-2014-129924|2014-07-12|2014-07-17|Standard Class|   AC-10420|FUR-TA-10004575|698.352|       3|     0.2|  -17.4588|\n",
            "|       590|US-2016-156986|2016-03-20|2016-03-24|Standard Class|   ZC-21910|OFF-PA-10002005| 20.736|       4|     0.2|    7.2576|\n",
            "|       684|US-2017-168116|2017-11-04|2017-11-04|      Same Day|   GT-14635|TEC-MA-10004125|7999.98|       4|     0.5|-3839.9904|\n",
            "|       739|CA-2014-132500|2014-09-08|2014-09-12|Standard Class|   GZ-14470|TEC-AC-10001383|  49.98|       2|     0.0|    8.4966|\n",
            "+----------+--------------+----------+----------+--------------+-----------+---------------+-------+--------+--------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identifying invalid dates\n",
        "\n",
        "sales_df=sales_df.withColumn(\"order_date\",to_date(col(\"order_date\"), \"yyyy-MM-dd\"))\n",
        "sales_df=sales_df.withColumn(\"ship_date\",to_date(col(\"ship_date\"), \"yyyy-MM-dd\"))\n",
        "invalid_dates = sales_df.filter((col(\"order_date\") > col(\"ship_date\"))|\n",
        "                                (col(\"order_date\").isNull())|\n",
        "                                (col(\"ship_date\").isNull())|\n",
        "                                (col(\"order_date\") > current_date())|\n",
        "                                (col(\"ship_date\") > current_date())\n",
        "                                )\n",
        "invalid_dates.show()\n",
        "\n",
        "\n",
        "logging.info(\"Invalid dates identified successfully\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0HtYxOsgewGZ",
        "outputId": "0fc38da0-7abb-4338-f01d-a653a5c25837"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+----------+---------+---------+-----------+----------+-----+--------+--------+------+\n",
            "|order_line|order_id|order_date|ship_date|ship_mode|customer_id|product_id|sales|quantity|discount|profit|\n",
            "+----------+--------+----------+---------+---------+-----------+----------+-----+--------+--------+------+\n",
            "+----------+--------+----------+---------+---------+-----------+----------+-----+--------+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filling the missing values with dafault\n",
        "\n",
        "customer_df= customer_df.fillna({\n",
        "    \"age\":0,\n",
        "    \"country\":\"unknown\",\n",
        "    \"state\":\"unknown\",\n",
        "    \"city\":\"unknown\",\n",
        "    \"postal_code\":0,\n",
        "    \"region\":\"unknown\"\n",
        "})\n",
        "products_df=products_df.fillna({\n",
        "    \"category\":\"unknown\",\n",
        "    \"sub_category\":\"unknown\",\n",
        "    \"product_name\":\"unknown\"\n",
        "})\n",
        "sales_df=sales_df.fillna({\n",
        "    \"ship_mode\":\"unknown\",\n",
        "    \"sales\":0.0,\n",
        "    \"quantity\":0,\n",
        "    \"discount\":0.0,\n",
        "    \"profit\":0.0\n",
        "})\n",
        "\n",
        "logging.info(\"Missing values filled successfully\")\n"
      ],
      "metadata": {
        "id": "oL3md9oPh5H2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join the DataFrames on relevant keys (Product ID and Customer ID).\n",
        "\n",
        "joined_df = sales_df.join(customer_df, on=\"customer_id\").join(products_df, on=\"product_id\")\n",
        "joined_df.show(5)\n",
        "\n",
        "logging.info(\"Dataframes joined successfully\")\n",
        "\n",
        "# Show the contents of log file\n",
        "!cat dmart.log\n",
        "\n",
        "joined_df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "k_TyyzXmjesa",
        "outputId": "fc2750a1-2a04-4ace-cf69-cbc8fa43c353"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-----------+----------+--------------+----------+----------+--------------+-------+--------+--------+----------+----------------+---------+---+-------------+-------------+--------------+-----------+-------+---------------+------------+--------------------+\n",
            "|     product_id|customer_id|order_line|      order_id|order_date| ship_date|     ship_mode|  sales|quantity|discount|    profit|   customer_name|  segment|age|      country|         city|         state|postal_code| region|       category|sub_category|        product_name|\n",
            "+---------------+-----------+----------+--------------+----------+----------+--------------+-------+--------+--------+----------+----------------+---------+---+-------------+-------------+--------------+-----------+-------+---------------+------------+--------------------+\n",
            "|OFF-ST-10002974|   TS-21610|       128|US-2017-107272|2017-11-05|2017-11-12|Standard Class|243.992|       7|     0.2|    30.499|    Troy Staebel| Consumer| 33|United States|      Phoenix|       Arizona|      85023|   West|Office Supplies|     Storage|Trav-L-File Heavy...|\n",
            "|FUR-TA-10004575|   AC-10420|       339|CA-2014-129924|2014-07-12|2014-07-17|Standard Class|698.352|       3|     0.2|  -17.4588|   Alyssa Crouse|Corporate| 69|United States|San Francisco|    California|      94122|   West|      Furniture|      Tables|Hon 5100 Series W...|\n",
            "|OFF-PA-10002005|   ZC-21910|       590|US-2016-156986|2016-03-20|2016-03-24|Standard Class| 20.736|       4|     0.2|    7.2576|Zuschuss Carroll| Consumer| 61|United States|        Salem|        Oregon|      97301|   West|Office Supplies|       Paper|           Xerox 225|\n",
            "|TEC-MA-10004125|   GT-14635|       684|US-2017-168116|2017-11-04|2017-11-04|      Same Day|7999.98|       4|     0.5|-3839.9904|  Grant Thornton|Corporate| 19|United States|   Burlington|North Carolina|      27217|  South|     Technology|    Machines|Cubify CubeX 3D P...|\n",
            "|TEC-AC-10001383|   GZ-14470|       739|CA-2014-132500|2014-09-08|2014-09-12|Standard Class|  49.98|       2|     0.0|    8.4966|   Gary Zandusky| Consumer| 64|United States|    Rochester|     Minnesota|      55901|Central|     Technology| Accessories|Logitech Wireless...|\n",
            "+---------------+-----------+----------+--------------+----------+----------+--------------+-------+--------+--------+----------+----------------+---------+---+-------------+-------------+--------------+-----------+-------+---------------+------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "2025-05-09 09:52:23,308 - INFO - Customer data loaded successfully\n",
            "2025-05-09 09:52:24,282 - INFO - Product data loaded successfully\n",
            "2025-05-09 09:52:25,218 - INFO - Sales data loaded successfully\n",
            "2025-05-09 09:52:25,219 - INFO - Customer DataFrame\n",
            "2025-05-09 09:52:25,694 - INFO - Products DataFrame\n",
            "2025-05-09 09:52:26,142 - INFO - Sales DataFrame\n",
            "2025-05-09 09:53:01,177 - INFO - Duplicates removed successfully\n",
            "2025-05-09 09:53:07,198 - INFO - Data types corrected successfully\n",
            "2025-05-09 09:53:20,020 - INFO - Null values identified successfully\n",
            "2025-05-09 09:53:29,145 - INFO - Null values removed successfully\n",
            "2025-05-09 09:53:34,442 - INFO - Column names renamed successfully\n",
            "2025-05-09 09:53:45,794 - INFO - Invalid dates identified successfully\n",
            "2025-05-09 09:53:55,225 - INFO - Missing values filled successfully\n",
            "2025-05-09 09:54:06,052 - INFO - Dataframes joined successfully\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9994"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a temporary view\n",
        "\n",
        "joined_df.createOrReplaceTempView(\"sales_data\")\n",
        "customer_df.createOrReplaceTempView(\"customer_data\")\n",
        "\n",
        "result=spark.sql(\"select * from sales_data\")\n",
        "result.show(5)\n",
        "logging.info(\"Temporary views created successfully\")\n",
        "result.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmnrEOGxuaaa",
        "outputId": "d11d80a8-e49c-4639-fa28-4a28f79fcbc7",
        "collapsed": true
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-----------+----------+--------------+----------+----------+--------------+-------+--------+--------+----------+----------------+---------+---+-------------+-------------+--------------+-----------+-------+---------------+------------+--------------------+\n",
            "|     product_id|customer_id|order_line|      order_id|order_date| ship_date|     ship_mode|  sales|quantity|discount|    profit|   customer_name|  segment|age|      country|         city|         state|postal_code| region|       category|sub_category|        product_name|\n",
            "+---------------+-----------+----------+--------------+----------+----------+--------------+-------+--------+--------+----------+----------------+---------+---+-------------+-------------+--------------+-----------+-------+---------------+------------+--------------------+\n",
            "|OFF-ST-10002974|   TS-21610|       128|US-2017-107272|2017-11-05|2017-11-12|Standard Class|243.992|       7|     0.2|    30.499|    Troy Staebel| Consumer| 33|United States|      Phoenix|       Arizona|      85023|   West|Office Supplies|     Storage|Trav-L-File Heavy...|\n",
            "|FUR-TA-10004575|   AC-10420|       339|CA-2014-129924|2014-07-12|2014-07-17|Standard Class|698.352|       3|     0.2|  -17.4588|   Alyssa Crouse|Corporate| 69|United States|San Francisco|    California|      94122|   West|      Furniture|      Tables|Hon 5100 Series W...|\n",
            "|OFF-PA-10002005|   ZC-21910|       590|US-2016-156986|2016-03-20|2016-03-24|Standard Class| 20.736|       4|     0.2|    7.2576|Zuschuss Carroll| Consumer| 61|United States|        Salem|        Oregon|      97301|   West|Office Supplies|       Paper|           Xerox 225|\n",
            "|TEC-MA-10004125|   GT-14635|       684|US-2017-168116|2017-11-04|2017-11-04|      Same Day|7999.98|       4|     0.5|-3839.9904|  Grant Thornton|Corporate| 19|United States|   Burlington|North Carolina|      27217|  South|     Technology|    Machines|Cubify CubeX 3D P...|\n",
            "|TEC-AC-10001383|   GZ-14470|       739|CA-2014-132500|2014-09-08|2014-09-12|Standard Class|  49.98|       2|     0.0|    8.4966|   Gary Zandusky| Consumer| 64|United States|    Rochester|     Minnesota|      55901|Central|     Technology| Accessories|Logitech Wireless...|\n",
            "+---------------+-----------+----------+--------------+----------+----------+--------------+-------+--------+--------+----------+----------------+---------+---+-------------+-------------+--------------+-----------+-------+---------------+------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9994"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################# Run Queries on the Pyspark #####################\n",
        "\n",
        "# 1.What is the total sales for each product category?\n",
        "\n",
        "# Using pyspark\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "joined_df.groupBy(\"category\") \\\n",
        "        .agg(F.round(F.sum(\"sales\"),2).alias(\"Total_Sales_in_Rupees\")) \\\n",
        "        .show()\n",
        "\n",
        "# Using spark sql\n",
        "result = spark.sql(\"SELECT category, round(SUM(sales),2) AS total_sales_in_Rupees FROM sales_data GROUP BY category\")\n",
        "result.show()\n",
        "logging.info(\"Query 1 executed successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiReHnN3ur_z",
        "outputId": "3900c781-073d-4042-84e1-f15204ccfd0b",
        "collapsed": true
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---------------------+\n",
            "|       category|Total_Sales_in_Rupees|\n",
            "+---------------+---------------------+\n",
            "|Office Supplies|            719047.03|\n",
            "|      Furniture|             741999.8|\n",
            "|     Technology|            836154.03|\n",
            "+---------------+---------------------+\n",
            "\n",
            "+---------------+---------------------+\n",
            "|       category|total_sales_in_Rupees|\n",
            "+---------------+---------------------+\n",
            "|Office Supplies|            719047.03|\n",
            "|      Furniture|             741999.8|\n",
            "|     Technology|            836154.03|\n",
            "+---------------+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Which customer has made the highest number of purchases?\n",
        "\n",
        "# Using Pyapark\n",
        "joined_df.groupBy(\"customer_name\") \\\n",
        "        .agg(F.count(\"order_id\").alias(\"purchase_count\")) \\\n",
        "        .orderBy(F.desc(\"purchase_count\")) \\\n",
        "        .limit(1) \\\n",
        "        .show()\n",
        "\n",
        "# Using spark sql\n",
        "result = spark.sql(\"SELECT customer_name, COUNT(order_id) AS purchase_count FROM sales_data GROUP BY customer_name ORDER BY purchase_count DESC LIMIT 1\")\n",
        "result.show()\n",
        "logging.info(\"Query 2 executed successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMR2gWAv2-nl",
        "outputId": "41d595c1-7d6b-42cc-df1e-e7a68ac958e6",
        "collapsed": true
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+--------------+\n",
            "|customer_name|purchase_count|\n",
            "+-------------+--------------+\n",
            "|William Brown|            37|\n",
            "+-------------+--------------+\n",
            "\n",
            "+-------------+--------------+\n",
            "|customer_name|purchase_count|\n",
            "+-------------+--------------+\n",
            "|William Brown|            37|\n",
            "+-------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. What is the average discount given on sales across all products?\n",
        "\n",
        "# using pyspark\n",
        "joined_df.agg(F.round(F.avg(\"discount\"),2).alias(\"Avg_Discount\")).show()\n",
        "\n",
        "# Using spark sql\n",
        "result = spark.sql(\"SELECT round(AVG(discount),2) AS average_discount FROM sales_data\")\n",
        "result.show()\n",
        "logging.info(\"Query 3 executed successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zl8gbYec4L25",
        "outputId": "bdd2a4a7-e54c-477e-d478-623988a373fa",
        "collapsed": true
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|Avg_Discount|\n",
            "+------------+\n",
            "|        0.16|\n",
            "+------------+\n",
            "\n",
            "+----------------+\n",
            "|average_discount|\n",
            "+----------------+\n",
            "|            0.16|\n",
            "+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. How many unique products were sold in each region?\n",
        "\n",
        "# Using Pyspark\n",
        "joined_df.groupBy(\"region\") \\\n",
        "        .agg(F.countDistinct(\"product_name\").alias(\"Unique_Products_Sold\")) \\\n",
        "        .show()\n",
        "\n",
        "# Using spark sql\n",
        "result = spark.sql(\"SELECT region, COUNT(DISTINCT(product_name)) AS unique_products FROM sales_data GROUP BY region\")\n",
        "result.show()\n",
        "logging.info(\"Query 4 executed successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XGfya229pXX",
        "outputId": "5d4cb0dd-1374-4df4-d252-14ba76167dfe",
        "collapsed": true
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+\n",
            "| region|Unique_Products_Sold|\n",
            "+-------+--------------------+\n",
            "|  South|                1037|\n",
            "|Central|                1289|\n",
            "|   East|                1374|\n",
            "|   West|                1505|\n",
            "+-------+--------------------+\n",
            "\n",
            "+-------+---------------+\n",
            "| region|unique_products|\n",
            "+-------+---------------+\n",
            "|  South|           1037|\n",
            "|Central|           1289|\n",
            "|   East|           1374|\n",
            "|   West|           1505|\n",
            "+-------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. What is the total profit generated in each state?\n",
        "\n",
        "# Using Pyspark\n",
        "joined_df.groupBy(\"state\") \\\n",
        "        .agg(F.round(F.sum(\"profit\"),2).alias(\"Total_Profit\")) \\\n",
        "        .show()\n",
        "\n",
        "# Using spark sql\n",
        "result = spark.sql(\"SELECT state, round(SUM(profit),2) AS total_profit FROM sales_data GROUP BY state\")\n",
        "result.show()\n",
        "logging.info(\"Query 5 executed successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pCMLoJJ-7h8",
        "outputId": "07b9ec50-3c25-4707-d6fb-96e2c84d2d60",
        "collapsed": true
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------------+\n",
            "|               state|Total_Profit|\n",
            "+--------------------+------------+\n",
            "|                Utah|     1818.19|\n",
            "|           Minnesota|     7202.52|\n",
            "|                Ohio|     5985.89|\n",
            "|              Oregon|      234.05|\n",
            "|            Arkansas|      -62.95|\n",
            "|               Texas|    20528.91|\n",
            "|        Pennsylvania|    13604.93|\n",
            "|         Connecticut|       533.5|\n",
            "|            Nebraska|     1166.02|\n",
            "|              Nevada|      278.07|\n",
            "|          Washington|     24405.8|\n",
            "|            Illinois|     9560.15|\n",
            "|            Oklahoma|      829.02|\n",
            "|District of Columbia|      490.96|\n",
            "|            Delaware|     3336.38|\n",
            "|          New Mexico|     1340.14|\n",
            "|            Missouri|     2212.87|\n",
            "|        Rhode Island|      2276.7|\n",
            "|             Georgia|    12781.34|\n",
            "|            Virginia|     6940.11|\n",
            "+--------------------+------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+--------------------+------------+\n",
            "|               state|total_profit|\n",
            "+--------------------+------------+\n",
            "|                Utah|     1818.19|\n",
            "|           Minnesota|     7202.52|\n",
            "|                Ohio|     5985.89|\n",
            "|              Oregon|      234.05|\n",
            "|            Arkansas|      -62.95|\n",
            "|               Texas|    20528.91|\n",
            "|        Pennsylvania|    13604.93|\n",
            "|         Connecticut|       533.5|\n",
            "|            Nebraska|     1166.02|\n",
            "|              Nevada|      278.07|\n",
            "|          Washington|     24405.8|\n",
            "|            Illinois|     9560.15|\n",
            "|            Oklahoma|      829.02|\n",
            "|District of Columbia|      490.96|\n",
            "|            Delaware|     3336.38|\n",
            "|          New Mexico|     1340.14|\n",
            "|            Missouri|     2212.87|\n",
            "|        Rhode Island|      2276.7|\n",
            "|             Georgia|    12781.34|\n",
            "|            Virginia|     6940.11|\n",
            "+--------------------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Which product sub-category has the highest sales?\n",
        "\n",
        "# Using Pyspark\n",
        "joined_df.groupBy(\"sub_category\") \\\n",
        "        .agg(F.round(F.sum(\"sales\"),2).alias(\"Total_Sales\")) \\\n",
        "        .orderBy(F.desc(\"Total_Sales\")) \\\n",
        "        .limit(1) \\\n",
        "        .show()\n",
        "\n",
        "# Using spark sql\n",
        "result = spark.sql(\"SELECT sub_category, round(SUM(sales),2) AS total_sales FROM sales_data GROUP BY sub_category ORDER BY total_sales DESC LIMIT 1\")\n",
        "result.show()\n",
        "logging.info(\"Query 6 executed successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BQiEnifAfS2",
        "outputId": "9ccaec29-bf78-4fe1-dfb0-f02be423b429",
        "collapsed": true
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+\n",
            "|sub_category|Total_Sales|\n",
            "+------------+-----------+\n",
            "|      Phones|  330007.05|\n",
            "+------------+-----------+\n",
            "\n",
            "+------------+-----------+\n",
            "|sub_category|total_sales|\n",
            "+------------+-----------+\n",
            "|      Phones|  330007.05|\n",
            "+------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. What is the average age of customers in each segment?\n",
        "\n",
        "# Using Pyspark\n",
        "joined_df.groupBy(\"segment\") \\\n",
        "        .agg(F.round(F.avg(\"age\"),2).alias(\"Average_Age\")) \\\n",
        "        .show()\n",
        "\n",
        "# Using spark sql\n",
        "result = spark.sql(\"SELECT segment, round(AVG(age),2) AS average_age FROM customer_data GROUP BY segment\")\n",
        "result.show()\n",
        "logging.info(\"Query 7 executed successfully\")\n"
      ],
      "metadata": {
        "id": "90CvH_OmBhLJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "bc1d087a-19d3-4d75-e999-f123fa81d697"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+\n",
            "|    segment|Average_Age|\n",
            "+-----------+-----------+\n",
            "|   Consumer|      44.61|\n",
            "|Home Office|      43.28|\n",
            "|  Corporate|      44.82|\n",
            "+-----------+-----------+\n",
            "\n",
            "+-----------+-----------+\n",
            "|    segment|average_age|\n",
            "+-----------+-----------+\n",
            "|   Consumer|       44.7|\n",
            "|Home Office|      43.81|\n",
            "|  Corporate|      44.47|\n",
            "+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. How many orders were shipped in each shipping mode?\n",
        "\n",
        "# Using Pyspark\n",
        "joined_df.groupBy(\"ship_mode\") \\\n",
        "        .agg(F.count(\"order_id\").alias(\"Order_Count\")) \\\n",
        "        .show()\n",
        "\n",
        "# Using spark sql\n",
        "result = spark.sql(\"SELECT ship_mode, COUNT(order_id) AS order_count FROM sales_data GROUP BY ship_mode\")\n",
        "result.show()\n",
        "logging.info(\"Query 8 executed successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cvw_3apRgHBi",
        "outputId": "117e6b3b-58c7-4ccc-85a1-06aa53bf3230"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+\n",
            "|     ship_mode|Order_Count|\n",
            "+--------------+-----------+\n",
            "|   First Class|       1538|\n",
            "|      Same Day|        543|\n",
            "|  Second Class|       1945|\n",
            "|Standard Class|       5968|\n",
            "+--------------+-----------+\n",
            "\n",
            "+--------------+-----------+\n",
            "|     ship_mode|order_count|\n",
            "+--------------+-----------+\n",
            "|   First Class|       1538|\n",
            "|      Same Day|        543|\n",
            "|  Second Class|       1945|\n",
            "|Standard Class|       5968|\n",
            "+--------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. What is the total quantity of products sold in each city?\n",
        "\n",
        "# Using Pyspark\n",
        "joined_df.groupBy(\"city\") \\\n",
        "        .agg(F.sum(\"quantity\").alias(\"Total_Quantity\")) \\\n",
        "        .show()\n",
        "\n",
        "# Using spark sql\n",
        "result = spark.sql(\"SELECT city, SUM(quantity) AS total_quantity FROM sales_data GROUP BY city\")\n",
        "result.show()\n",
        "logging.info(\"Query 9 executed successfully\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vmojkR1vyHa",
        "outputId": "e6dcd9c6-ec4a-44af-9c82-cb91838a6b5f",
        "collapsed": true
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------------+\n",
            "|           city|Total_Quantity|\n",
            "+---------------+--------------+\n",
            "|          Tyler|            22|\n",
            "|    Springfield|           282|\n",
            "|  Bowling Green|            89|\n",
            "|         Auburn|           214|\n",
            "|North Las Vegas|            29|\n",
            "|        Phoenix|           256|\n",
            "|  Lake Elsinore|            35|\n",
            "|         Monroe|           184|\n",
            "| Pembroke Pines|            48|\n",
            "|       Westland|            54|\n",
            "|    Lindenhurst|            42|\n",
            "|         Marion|            77|\n",
            "|          Omaha|            43|\n",
            "|   Fort Collins|            55|\n",
            "|        Everett|            24|\n",
            "|     Greensboro|            51|\n",
            "|   Lincoln Park|            28|\n",
            "|       Franklin|           184|\n",
            "|         Dallas|           602|\n",
            "|      Encinitas|            87|\n",
            "+---------------+--------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+---------------+--------------+\n",
            "|           city|total_quantity|\n",
            "+---------------+--------------+\n",
            "|          Tyler|            22|\n",
            "|    Springfield|           282|\n",
            "|  Bowling Green|            89|\n",
            "|         Auburn|           214|\n",
            "|North Las Vegas|            29|\n",
            "|        Phoenix|           256|\n",
            "|  Lake Elsinore|            35|\n",
            "|         Monroe|           184|\n",
            "| Pembroke Pines|            48|\n",
            "|       Westland|            54|\n",
            "|    Lindenhurst|            42|\n",
            "|         Marion|            77|\n",
            "|          Omaha|            43|\n",
            "|   Fort Collins|            55|\n",
            "|        Everett|            24|\n",
            "|     Greensboro|            51|\n",
            "|   Lincoln Park|            28|\n",
            "|       Franklin|           184|\n",
            "|         Dallas|           602|\n",
            "|      Encinitas|            87|\n",
            "+---------------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Which customer segment has the highest profit margin?\n",
        "\n",
        "# Using Pyspark\n",
        "joined_df.groupBy(\"segment\") \\\n",
        "        .agg(F.round(F.sum(\"profit\")/F.sum(\"sales\"),2).alias(\"Profit_Margin\")) \\\n",
        "        .orderBy(F.desc(\"Profit_Margin\")) \\\n",
        "        .limit(1) \\\n",
        "        .show()\n",
        "\n",
        "# Using spark sql\n",
        "result = spark.sql(\"SELECT segment, round(SUM(profit)/SUM(sales),2) AS profit_margin FROM sales_data GROUP BY segment ORDER BY profit_margin DESC LIMIT 1\")\n",
        "result.show()\n",
        "logging.info(\"Query 10 executed successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bhW3TRSyh_2m",
        "outputId": "cb42b9dd-d088-4e05-f091-47da54a49e33"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+\n",
            "|    segment|Profit_Margin|\n",
            "+-----------+-------------+\n",
            "|Home Office|         0.14|\n",
            "+-----------+-------------+\n",
            "\n",
            "+-----------+-------------+\n",
            "|    segment|profit_margin|\n",
            "+-----------+-------------+\n",
            "|Home Office|         0.14|\n",
            "+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# log file\n",
        "!cat dmart.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YEFyvO_onTd3",
        "outputId": "6e67feec-9f8e-411a-ea3a-0ccbe3406bc3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-09 09:52:23,308 - INFO - Customer data loaded successfully\n",
            "2025-05-09 09:52:24,282 - INFO - Product data loaded successfully\n",
            "2025-05-09 09:52:25,218 - INFO - Sales data loaded successfully\n",
            "2025-05-09 09:52:25,219 - INFO - Customer DataFrame\n",
            "2025-05-09 09:52:25,694 - INFO - Products DataFrame\n",
            "2025-05-09 09:52:26,142 - INFO - Sales DataFrame\n",
            "2025-05-09 09:53:01,177 - INFO - Duplicates removed successfully\n",
            "2025-05-09 09:53:07,198 - INFO - Data types corrected successfully\n",
            "2025-05-09 09:53:20,020 - INFO - Null values identified successfully\n",
            "2025-05-09 09:53:29,145 - INFO - Null values removed successfully\n",
            "2025-05-09 09:53:34,442 - INFO - Column names renamed successfully\n",
            "2025-05-09 09:53:45,794 - INFO - Invalid dates identified successfully\n",
            "2025-05-09 09:53:55,225 - INFO - Missing values filled successfully\n",
            "2025-05-09 09:54:06,052 - INFO - Dataframes joined successfully\n",
            "2025-05-09 09:54:17,878 - INFO - Temporary views created successfully\n",
            "2025-05-09 09:54:28,078 - INFO - Query 1 executed successfully\n",
            "2025-05-09 09:54:42,380 - INFO - Query 2 executed successfully\n",
            "2025-05-09 09:55:03,373 - INFO - Query 4 executed successfully\n",
            "2025-05-09 09:55:25,729 - INFO - Query 3 executed successfully\n",
            "2025-05-09 09:55:38,506 - INFO - Query 5 executed successfully\n",
            "2025-05-09 09:55:50,447 - INFO - Query 6 executed successfully\n",
            "2025-05-09 09:56:05,006 - INFO - Query 7 executed successfully\n",
            "2025-05-09 09:56:14,679 - INFO - Query 8 executed successfully\n",
            "2025-05-09 09:56:26,397 - INFO - Query 9 executed successfully\n",
            "2025-05-09 09:56:39,615 - INFO - Query 10 executed successfully\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}